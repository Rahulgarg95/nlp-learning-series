{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f595465",
   "metadata": {},
   "source": [
    "# Advanced Text Preprocessing Fundamentals for NLP 🧬\n",
    "\n",
    "**Description**: Dive deep into advanced text preprocessing, solving challenges like code-switching, emoji handling, context-aware normalization, multi-lingual pipelines, social/text dialects, dependency parsing for preprocessing, specialized regular expressions, and transformer-friendly formatting. 🧠🛠️\n",
    "\n",
    "***\n",
    "\n",
    "## Table of Contents\n",
    "- 1. Why Advanced Preprocessing? 💡\n",
    "- 2. Unicode, Text Noise & Exotic Scripts 🌐\n",
    "- 3. Regex Mastery & Adversarial Patterns 🧩\n",
    "- 4. Social Text & Emoji Handling 💬🙂\n",
    "- 5. Code-switching & Multilingual Contexts 🔀🌍\n",
    "- 6. POS, Morphology & Dependency-driven Cleaning 🧠🧭\n",
    "- 7. Context-aware Normalization (Rule-based & ML-based) ⚖️🤖\n",
    "- 8. Handling Outlier/Niche Text: URLs, Dates, Numbers, Code Snippets 📎📅🔢💻\n",
    "\n",
    "***\n",
    "\n",
    "## 1. Why Advanced Preprocessing? 💡\n",
    "Traditional cleaning is not enough for real-world applications (social media, legal/medical, code-mixed, low-resource, web-scraped text).  \n",
    "Focus on preserving signal while minimizing noise so downstream models remain accurate and robust. 🎯\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1176a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text analytics setup: imports, model loading, and tokenization utilities.\n",
    "\n",
    "This module centralizes common NLP dependencies (spaCy, NLTK, etc.), provides\n",
    "safe loading for spaCy language models with helpful guidance, and initializes a\n",
    "tweet-friendly tokenizer for robust social/text processing.\n",
    "\n",
    "Usage:\n",
    "- Ensure spaCy models are installed (see one-time setup below).\n",
    "- Import EN_NLP / ES_NLP for language processing pipelines.\n",
    "- Use tknzr for Twitter-aware tokenization.\n",
    "\"\"\"\n",
    "\n",
    "# Standard library\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Third-party: data and NLP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import langid\n",
    "import spacy\n",
    "from spacy.language import Language  # for type hints\n",
    "\n",
    "# NLTK: tokenization & stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# One-time setup (uncomment and run if models/data are missing)\n",
    "# -----------------------------------------------------------------------------\n",
    "# spaCy small English and Spanish models:\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download es_core_news_sm\n",
    "#\n",
    "# NLTK resources (if you get LookupError for tokenizers/stopwords):\n",
    "# import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "# Matplotlib style for consistent, clean visuals (adjust to preference)\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "# spaCy model names centralized as constants for easy change/override\n",
    "EN_MODEL_NAME = \"en_core_web_sm\"\n",
    "ES_MODEL_NAME = \"es_core_news_sm\"\n",
    "\n",
    "\n",
    "def _load_spacy_model(name: str) -> Language:\n",
    "    \"\"\"\n",
    "    Load a spaCy model by name with a helpful error if it's not installed.\n",
    "\n",
    "    Parameters:\n",
    "        name: The registered spaCy model name (e.g., \"en_core_web_sm\").\n",
    "\n",
    "    Returns:\n",
    "        A spaCy Language pipeline.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If the model is not installed, with guidance to install it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spacy.load(name)\n",
    "    except OSError as exc:\n",
    "        raise OSError(\n",
    "            f\"spaCy model '{name}' is not installed. \"\n",
    "            f\"Install it with: python -m spacy download {name}\"\n",
    "        ) from exc\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# NLP Pipelines & Tokenizer\n",
    "# -----------------------------------------------------------------------------\n",
    "# English and Spanish NLP pipelines (small models; fast and lightweight)\n",
    "EN_NLP: Language = _load_spacy_model(EN_MODEL_NAME)\n",
    "ES_NLP: Language = _load_spacy_model(ES_MODEL_NAME)\n",
    "\n",
    "# Tweet-aware tokenizer that handles mentions, hashtags, emojis, and URLs well\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# Optional: pre-load stopwords for convenience (wrap in try to avoid LookupError)\n",
    "try:\n",
    "    EN_STOPWORDS = set(stopwords.words(\"english\"))\n",
    "except LookupError:\n",
    "    EN_STOPWORDS = set()\n",
    "    # Tip: run the NLTK downloads in the setup block above if this is empty.\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Quick sanity checks (comment out in production)\n",
    "# -----------------------------------------------------------------------------\n",
    "# doc = EN_NLP(\"Hello world! This is a quick spaCy sanity check.\")\n",
    "# print([t.text for t in doc])\n",
    "# print(tknzr.tokenize(\"Testing @mentions, #hashtags, and emojis 😄 http://example.com\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8b4e3",
   "metadata": {},
   "source": [
    "## 2. Unicode, Text Noise & Exotic Scripts 🌐\n",
    "Normalize Unicode (NFC/NFKC), strip zero-widths, and manage mixed scripts safely for consistent text flow.  \n",
    "Sanitize HTML/XML, normalize whitespace, and define policies for removing or replacing non-text artifacts. 🧹\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562c284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW: b'Caf\\\\xe9 M\\\\xfcnster \\\\u2014 \\\\u041f\\\\u0440\\\\u0438\\\\u0432\\\\u0435\\\\u0442! \\\\u2014 \\\\u062a\\\\u064e\\\\u062c\\\\u0652\\\\u0631\\\\u0650\\\\u0628\\\\u0629 \\\\U0001f9d1\\\\U0001f3fd\\\\u200d\\\\U0001f4bb\\\\u202e\\\\n\\\\n'\n",
      "UNICODE CLEAN: Café Münster — Привет! — تَجْرِبة 🧑🏽💻\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Café Münster — Привет! — تَجْرِبة 🧑🏽‍💻\\u202e\\n\\n\"\n",
    "print('RAW:', sample_text.encode('unicode_escape'))\n",
    "\n",
    "# Remove control chars, normalize unicode, strip bidirectional & zero-width\n",
    "import unicodedata\n",
    "\n",
    "def clean_unicode(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = ''.join(ch for ch in text if not unicodedata.category(ch).startswith('C'))\n",
    "    text = re.sub(r'[\\u200e\\u202e\\u200b\\u200c\\u200d]', '', text)  # Remove ZW and Bidi\n",
    "    return text\n",
    "\n",
    "print('UNICODE CLEAN:', clean_unicode(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffccd40c",
   "metadata": {},
   "source": [
    "## 3. Regex Mastery & Adversarial Patterns 🧩\n",
    "Use explicit, compiled patterns with unit tests; avoid greedy matches and catastrophic backtracking.  \n",
    "Design for adversarial inputs (nested brackets, quotes, long runs) and validate with fuzz tests. 🧪\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2ba3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails: ['john.doe@email.com', 'john.doe@email.com']\n",
      "URLs: ['https://domain.com']\n",
      "Phones: ['(555) 123-4567']\n",
      "Emojis: ['😊']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Robust extractors for emails, URLs (incl. Markdown), phone numbers, and emojis.\n",
    "- Handles Markdown links: [label](https://example.com)\n",
    "- Avoids trailing punctuation in URLs (e.g., ')', ']', '>')\n",
    "- Uses compiled regex for performance and clarity\n",
    "- Updated for emoji>=2.0 (no get_emoji_regexp); uses emoji_list with a safe fallback\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Sample text\n",
    "text = (\n",
    "    \"Contact: [john.doe@email.com](mailto:john.doe@email.com), \"\n",
    "    \"Call: (555) 123-4567, \"\n",
    "    \"Visit: [https://domain.com](https://domain.com) @user #hashtag 😊\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Compiled regex patterns\n",
    "# -----------------------------\n",
    "# Email: reasonably permissive for extraction (not strict validation)\n",
    "EMAIL_RE = re.compile(r\"[A-Za-z0-9.!#$%&'*+/=?^_`{|}~-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "\n",
    "# URL (raw): stop at whitespace or common closing punctuation in prose/Markdown\n",
    "URL_RE = re.compile(r\"https?://[^\\s\\])>]+\")\n",
    "\n",
    "# Markdown links: capture label and target URL separately\n",
    "MD_LINK_RE = re.compile(r\"\\[([^\\]]+)\\]\\((https?://[^)]+)\\)\")\n",
    "\n",
    "# Phone (US-like): optional parentheses, separators (space, dash, dot), optional country code\n",
    "PHONE_RE = re.compile(\n",
    "    r\"(?:\\+?\\d{1,3}[\\s.-])?\"        # optional country code, e.g., +1\n",
    "    r\"(?:\\(?\\d{3}\\)?[\\s.-])\"        # area code with optional parentheses\n",
    "    r\"\\d{3}[\\s.-]\\d{4}\"             # local number\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Emoji extraction (emoji>=2.0)\n",
    "# -----------------------------\n",
    "def extract_emojis(txt: str):\n",
    "    \"\"\"\n",
    "    Prefer emoji.emoji_list (emoji>=2). Fallback to a compiled regex built\n",
    "    from emoji.EMOJI_DATA keys if emoji_list is unavailable.\n",
    "    \"\"\"\n",
    "    if hasattr(emoji, \"emoji_list\"):\n",
    "        # [{'emoji': '😊', 'match_start': 123, 'match_end': 124}, ...] -> just the glyphs\n",
    "        return [d[\"emoji\"] for d in emoji.emoji_list(txt)]\n",
    "    else:\n",
    "        # Older environments: build a regex from known emojis; sort by length to match sequences first\n",
    "        try:\n",
    "            from emoji import EMOJI_DATA\n",
    "            emoji_re = re.compile(\n",
    "                \"|\".join(re.escape(e) for e in sorted(EMOJI_DATA.keys(), key=len, reverse=True))\n",
    "            )\n",
    "            return emoji_re.findall(txt)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "# -----------------------------\n",
    "# Extraction\n",
    "# -----------------------------\n",
    "# 1) Emails\n",
    "emails = EMAIL_RE.findall(text)\n",
    "\n",
    "# 2) URLs\n",
    "#    - Extract Markdown URLs first (both label and target)\n",
    "md_links = MD_LINK_RE.findall(text)  # list of tuples: (label, url)\n",
    "md_urls = [u for _, u in md_links]\n",
    "\n",
    "#    - Extract raw URLs not wrapped in Markdown\n",
    "raw_urls = URL_RE.findall(text)\n",
    "\n",
    "#    - Combine and de-duplicate while preserving order\n",
    "seen = set()\n",
    "urls = []\n",
    "for u in md_urls + raw_urls:\n",
    "    if u not in seen:\n",
    "        urls.append(u)\n",
    "        seen.add(u)\n",
    "\n",
    "# 3) Phones\n",
    "phones = PHONE_RE.findall(text)\n",
    "\n",
    "# 4) Emojis (emoji>=2 preferred path)\n",
    "emojis = extract_emojis(text)\n",
    "\n",
    "# -----------------------------\n",
    "# Output\n",
    "# -----------------------------\n",
    "print(f\"Emails: {emails}\")\n",
    "print(f\"URLs: {urls}\")\n",
    "print(f\"Phones: {phones}\")\n",
    "print(f\"Emojis: {emojis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a090",
   "metadata": {},
   "source": [
    "## 4. Social Text & Emoji Handling 💬🙂\n",
    "Adopt emoji-aware tokenizers; map emojis to sentiment/intent where useful instead of blanket removal.  \n",
    "Preserve hashtags and mentions as features; separate display needs from modeling via normalization. #️⃣👤\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c5e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm EMOJI: Loving this! face_with_tears_of_joysmiling_face_with_heart-eyesfire Visit https://t.co/abc123 @nlp_guru #NLP #AI lmaooo\n",
      "Cleaned Social: Loving this! face_with_tears_of_joysmiling_face_with_heart-eyesfire Visit  user  NLP AI lmaoo\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "def normalize_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(\"\", \"\"))\n",
    "\n",
    "def clean_social(text):\n",
    "    text = re.sub(r'@\\w+', ' user ', text)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)  # Remove hashtag but keep keyword\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r\"\\b(?:lol|omg|lmao)\\b\", \"laugh\", text, flags=re.I)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    return text\n",
    "\n",
    "tweet = \"Loving this! 😂😍🔥 Visit https://t.co/abc123 @nlp_guru #NLP #AI lmaooo\"\n",
    "print(\"Norm EMOJI:\", normalize_emojis(tweet))\n",
    "print(\"Cleaned Social:\", clean_social(normalize_emojis(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c36d9",
   "metadata": {},
   "source": [
    "## 5. Code-switching & Multilingual Contexts 🔀🌍\n",
    "Detect language at segment or sentence level and route through language-specific pipelines.  \n",
    "For code-mixed text, combine tokenization strategies and locale-aware normalization without collapsing scripts. 🧭\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b83ead7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I need ayuda with this task.' -- LANG: en (-96.63)\n",
      "[{'text': 'I', 'pos': 'PRON'}, {'text': 'need', 'pos': 'VERB'}, {'text': 'ayuda', 'pos': 'NOUN'}, {'text': 'with', 'pos': 'ADP'}, {'text': 'this', 'pos': 'DET'}, {'text': 'task', 'pos': 'NOUN'}, {'text': '.', 'pos': 'PUNCT'}]\n",
      "'Vamos to the park after work.' -- LANG: en (-110.23)\n",
      "[{'text': 'Vamos', 'pos': 'PROPN'}, {'text': 'to', 'pos': 'ADP'}, {'text': 'the', 'pos': 'DET'}, {'text': 'park', 'pos': 'NOUN'}, {'text': 'after', 'pos': 'ADP'}, {'text': 'work', 'pos': 'NOUN'}, {'text': '.', 'pos': 'PUNCT'}]\n",
      "'Bonjour! How are you doing?' -- LANG: en (-41.62)\n",
      "[{'text': 'Bonjour', 'pos': 'NOUN'}, {'text': '!', 'pos': 'PUNCT'}, {'text': 'How', 'pos': 'SCONJ'}, {'text': 'are', 'pos': 'AUX'}, {'text': 'you', 'pos': 'PRON'}, {'text': 'doing', 'pos': 'VERB'}, {'text': '?', 'pos': 'PUNCT'}]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"I need ayuda with this task.\",\n",
    "         \"Vamos to the park after work.\",\n",
    "         \"Bonjour! How are you doing?\"]\n",
    "\n",
    "for t in texts:\n",
    "    lang, prob = langid.classify(t)\n",
    "    print(f\"{repr(t)} -- LANG: {lang} ({prob:.2f})\")\n",
    "    if lang == \"en\":\n",
    "        doc = EN_NLP(t)\n",
    "    elif lang == \"es\":\n",
    "        doc = ES_NLP(t)\n",
    "    else:\n",
    "        doc = EN_NLP(t)\n",
    "    print([{'text': token.text, 'pos': token.pos_} for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c3cbe",
   "metadata": {},
   "source": [
    "## 6. POS, Morphology & Dependency-driven Cleaning 🧠🧭\n",
    "Lemmatize with POS awareness; avoid lowercasing when casing carries signal (NER, acronyms).  \n",
    "Use dependencies to trim boilerplate spans while keeping relation-bearing tokens intact. 🧱\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a11e240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas (verbs lemmatized): Dr. Adams prescribe 10 mg Ibuprofen to John Smith at 4 pm .\n",
      "Masked text: Dr. <PERSON> prescribed 10mg Ibuprofen to <PERSON> at 4pm.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lemmatize verbs and mask PII spans (PERSON, ORG) using spaCy.\n",
    "\n",
    "- Verbs are lemmatized while other tokens remain unchanged.\n",
    "- PII masking replaces full entity spans (e.g., \"John Smith\") with a single placeholder (e.g., \"<PERSON>\").\n",
    "- Character-span masking preserves original whitespace and punctuation.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Iterable, Tuple\n",
    "import spacy\n",
    "\n",
    "# Assumes EN_NLP is already loaded elsewhere:\n",
    "# EN_NLP = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "TEXT = \"Dr. Adams prescribed 10mg Ibuprofen to John Smith at 4pm.\"\n",
    "MASK_LABELS = (\"PERSON\", \"ORG\")\n",
    "PLACEHOLDER_BY_LABEL = {\n",
    "    \"PERSON\": \"<PERSON>\",\n",
    "    \"ORG\": \"<ORG>\",\n",
    "}\n",
    "DEFAULT_PLACEHOLDER = \"<MASK>\"\n",
    "\n",
    "\n",
    "def lemmatize_verbs(doc: spacy.tokens.Doc) -> str:\n",
    "    \"\"\"Return a string where only verbs are lemmatized.\"\"\"\n",
    "    return \" \".join(tok.lemma_ if tok.pos_ == \"VERB\" else tok.text for tok in doc)\n",
    "\n",
    "\n",
    "def _entity_spans_for_labels(\n",
    "    doc: spacy.tokens.Doc, labels: Iterable[str]\n",
    ") -> Iterable[Tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    Collect (start_char, end_char, label_) for entities matching given labels.\n",
    "    spaCy ensures entity spans do not overlap; sorting reverse avoids index shift on replacement.\n",
    "    \"\"\"\n",
    "    spans = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents if ent.label_ in labels]\n",
    "    spans.sort(key=lambda x: x[0], reverse=True)\n",
    "    return spans\n",
    "\n",
    "\n",
    "def mask_pii_spans(\n",
    "    text: str,\n",
    "    spans: Iterable[Tuple[int, int, str]],\n",
    "    placeholder_by_label: dict,\n",
    "    default_placeholder: str = \"<MASK>\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Replace each entity span with a single placeholder token.\n",
    "    Replacement occurs from right to left to keep indices valid.\n",
    "    \"\"\"\n",
    "    masked = text\n",
    "    for start, end, label in spans:\n",
    "        repl = placeholder_by_label.get(label, default_placeholder)\n",
    "        masked = masked[:start] + repl + masked[end:]\n",
    "    return masked\n",
    "\n",
    "\n",
    "# Process\n",
    "doc = EN_NLP(TEXT)\n",
    "\n",
    "# 1) Lemmatize only verbs\n",
    "lemmas_str = lemmatize_verbs(doc)\n",
    "\n",
    "# 2) Mask PII entity spans\n",
    "spans = _entity_spans_for_labels(doc, MASK_LABELS)\n",
    "masked_text = mask_pii_spans(TEXT, spans, PLACEHOLDER_BY_LABEL, DEFAULT_PLACEHOLDER)\n",
    "\n",
    "print(\"Lemmas (verbs lemmatized):\", lemmas_str)\n",
    "print(\"Masked text:\", masked_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb8244",
   "metadata": {},
   "source": [
    "## 7. Context-aware Normalization (Rule-based & ML-based) ⚖️🤖\n",
    "Blend curated rules (domain shorthands, units) with ML normalizers for ambiguous cases.  \n",
    "Version rulesets, log diffs, and A/B their impact on downstream metrics before rollout. 📈\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c3b1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not sure if patient's dose is high. please discontinue immediately!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Smart text normalization:\n",
    "- Expands English contractions (e.g., \"I'm\" -> \"I am\").\n",
    "- Normalizes common clinical shorthand (pt, pls, d/c, stat).\n",
    "- Preserves possessives (e.g., \"pt's\" -> \"patient's\").\n",
    "- Cleans extra spaces before punctuation.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "from typing import Literal\n",
    "\n",
    "def smart_normalize(\n",
    "    text: str,\n",
    "    dc_meaning: Literal[\"discontinue\", \"discharge\"] = \"discontinue\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Normalize text by expanding contractions and replacing domain-specific slang.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input text to normalize.\n",
    "    dc_meaning : {\"discontinue\", \"discharge\"}, optional\n",
    "        Intended meaning for the shorthand \"d/c\" (default: \"discontinue\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Normalized text.\n",
    "    \"\"\"\n",
    "    # 1) Expand standard English contractions\n",
    "    s = contractions.fix(text)\n",
    "\n",
    "    # 2) Domain-specific replacements (case-insensitive)\n",
    "    #    Order matters (handle possessive \"pt's\" before bare \"pt\")\n",
    "    rules = [\n",
    "        (re.compile(r\"\\bpt's\\b\", flags=re.IGNORECASE), \"patient's\"),\n",
    "        (re.compile(r\"\\bpt\\b\", flags=re.IGNORECASE), \"patient\"),\n",
    "        (re.compile(r\"\\bpls\\b\", flags=re.IGNORECASE), \"please\"),\n",
    "        (re.compile(r\"\\b(?:d\\s*/\\s*c)\\b\", flags=re.IGNORECASE), dc_meaning),\n",
    "        (re.compile(r\"\\bstat\\b\", flags=re.IGNORECASE), \"immediately\"),\n",
    "    ]\n",
    "    for pattern, replacement in rules:\n",
    "        s = pattern.sub(replacement, s)\n",
    "\n",
    "    # 3) Tidy whitespace around punctuation and collapse multiple spaces\n",
    "    s = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", s)   # no space before punctuation\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()    # collapse runs of spaces\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "# Example\n",
    "print(smart_normalize(\"I'm not sure if pt's dose is high. Pls d/c stat!\"))\n",
    "# -> \"I am not sure if patient's dose is high. Please discontinue immediately!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7a23a",
   "metadata": {},
   "source": [
    "## 8. Handling Outlier/Niche Text: URLs, Dates, Numbers, Code Snippets 📎📅🔢💻\n",
    "Prefer dedicated parsers for URLs, dates, and numbers; avoid brittle regex where standards exist.  \n",
    "Fence code blocks and tokenize separately to prevent leakage into language features. 🔒\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b1b8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates: ['2023-07-15']\n",
      "Times: ['7:45am']\n",
      "Code: [\"print('Hello, world!')\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "example = \"Glucose: 98mg/dl @ 7:45am on 2023-07-15. Use print('Hello, world!') to debug.\"\n",
    "\n",
    "# Compile patterns\n",
    "DATE_RE = re.compile(r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\")\n",
    "TIME_RE = re.compile(r\"\\b(?:[01]?\\d|2[0-3]):[0-5]\\d\\s?(?:am|pm)?\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "# Simple, non-greedy print(...) extraction (handles common cases without nesting)\n",
    "PRINT_RE = re.compile(r\"print\\([^)]*?\\)\")\n",
    "\n",
    "# Extract\n",
    "find_dates = DATE_RE.findall(example)\n",
    "find_times = TIME_RE.findall(example)\n",
    "find_code  = PRINT_RE.findall(example)\n",
    "\n",
    "print(\"Dates:\", find_dates)\n",
    "print(\"Times:\", find_times)\n",
    "print(\"Code:\", find_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
